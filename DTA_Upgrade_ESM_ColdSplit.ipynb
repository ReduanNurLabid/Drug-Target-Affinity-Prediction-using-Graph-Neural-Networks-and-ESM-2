{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c352c5a",
      "metadata": {
        "id": "8c352c5a"
      },
      "source": [
        "# Drug-Target Affinity Model Upgrade (Colab Edition)\n",
        "\n",
        "This notebook implements the following upgrades:\n",
        "1. **ESM-2 Protein Embeddings**: Replacing CNNs with a 650M parameter Language Model.\n",
        "2. **Bi-Directional Cross-Attention**: Modeling interaction between Drug Graph and Protein Sequence atoms.\n",
        "3. **Cold-Split Evaluation**: Rigorous testing on unseen drugs and targets.\n",
        "\n",
        "## Colab Setup\n",
        "Run the following cells to set up the environment on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f728c06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f728c06",
        "outputId": "7f5dd1c2-4d0e-47c3-c4d2-6f3e63a664c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit, torch-geometric\n",
            "Successfully installed rdkit-2025.9.3 torch-geometric-2.7.0\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# 1. Install Dependencies\n",
        "!pip install torch-geometric transformers rdkit pandas matplotlib seaborn\n",
        "\n",
        "# Check Device\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Mount Drive and Set Data Path\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Path to your 'data' folder on Drive\n",
        "DATA_DIR = '/content/drive/MyDrive/443/termproject/data'\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"WARNING: {DATA_DIR} does not exist. Please check path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stV7uObrbR6N",
        "outputId": "ccf63413-ac2b-4f5c-9365-3c686c651176"
      },
      "id": "stV7uObrbR6N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load and Verify Data Files\n",
        "import pandas as pd\n",
        "\n",
        "# Define file paths\n",
        "DRUGS_PATH = os.path.join(DATA_DIR, \"drugs.csv\")\n",
        "PROTEINS_PATH = os.path.join(DATA_DIR, \"proteins.csv\")\n",
        "AFFINITY_PATH = os.path.join(DATA_DIR, \"drug_protein_affinity.csv\")\n",
        "\n",
        "# Verify files exist\n",
        "for path in [DRUGS_PATH, PROTEINS_PATH, AFFINITY_PATH]:\n",
        "    if os.path.exists(path):\n",
        "        print(f\"✓ Found: {os.path.basename(path)}\")\n",
        "    else:\n",
        "        print(f\"✗ MISSING: {path}\")\n",
        "\n",
        "# Load and show basic info\n",
        "print(\"\\n--- Data Summary ---\")\n",
        "drugs_df = pd.read_csv(DRUGS_PATH)\n",
        "proteins_df = pd.read_csv(PROTEINS_PATH)\n",
        "affinity_df = pd.read_csv(AFFINITY_PATH)\n",
        "\n",
        "print(f\"Drugs: {len(drugs_df)} compounds\")\n",
        "print(f\"Proteins: {len(proteins_df)} targets\")\n",
        "print(f\"Affinity pairs: {len(affinity_df)} interactions\")\n",
        "\n",
        "# Preview\n",
        "print(\"\\n--- Drugs Sample ---\")\n",
        "print(drugs_df.head(2))\n",
        "print(\"\\n--- Proteins Sample ---\")\n",
        "print(proteins_df.head(2))\n",
        "print(\"\\n--- Affinity Sample ---\")\n",
        "print(affinity_df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8evSeT8ghIx",
        "outputId": "10ecfa4b-e381-42ef-b229-bb2a6bb07847"
      },
      "id": "J8evSeT8ghIx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Found: drugs.csv\n",
            "✓ Found: proteins.csv\n",
            "✓ Found: drug_protein_affinity.csv\n",
            "\n",
            "--- Data Summary ---\n",
            "Drugs: 68 compounds\n",
            "Proteins: 433 targets\n",
            "Affinity pairs: 29444 interactions\n",
            "\n",
            "--- Drugs Sample ---\n",
            "   Drug_Index       CID                                   Canonical_SMILES  \\\n",
            "0           0  11314340  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
            "1           1  24889392  CC(C)(C)C1=CC(=NO1)NC(=O)NC2=CC=C(C=C2)C3=CN4C...   \n",
            "\n",
            "                                     Isomeric_SMILES  \n",
            "0  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OC[C@H](CC4=C...  \n",
            "1  CC(C)(C)C1=CC(=NO1)NC(=O)NC2=CC=C(C=C2)C3=CN4C...  \n",
            "\n",
            "--- Proteins Sample ---\n",
            "   Protein_Index Accession_Number                   Gene_Name  \\\n",
            "0              0      NP_055726.3                        AAK1   \n",
            "1              1      NP_005148.2  ABL1(E255K)-phosphorylated   \n",
            "\n",
            "                                            Sequence  \n",
            "0  MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...  \n",
            "1  MLEICLKLVGCKSKKGLSSSSSCYLEEALQRPVASDFEPQGLSEAA...  \n",
            "\n",
            "--- Affinity Sample ---\n",
            "   Drug_Index  Protein_Index  Affinity\n",
            "0           0              0  7.366532\n",
            "1           0              1  5.000000\n",
            "2           0              3  5.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Utility Functions - SMILES to Molecular Graph\n",
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return [x == s for s in allowable_set]\n",
        "\n",
        "def atom_features(atom):\n",
        "    \"\"\"Get atom features: atomic num, degree, hydrogens (one-hot encoded)\"\"\"\n",
        "    return np.array(one_of_k_encoding(atom.GetAtomicNum(), [6, 7, 8, 9, 15, 16, 17, 35, 53, 0]) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
        "                    one_of_k_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]), dtype=np.float32)\n",
        "\n",
        "def smile_to_graph(smile):\n",
        "    \"\"\"Convert SMILES string to PyG Data object\"\"\"\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "    if mol is None:\n",
        "        return None\n",
        "\n",
        "    # Node features\n",
        "    num_atoms = mol.GetNumAtoms()\n",
        "    features = np.array([atom_features(atom) for atom in mol.GetAtoms()])\n",
        "\n",
        "    # Edge indices\n",
        "    edges = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edges.append([i, j])\n",
        "        edges.append([j, i])  # Undirected\n",
        "\n",
        "    if len(edges) == 0:\n",
        "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
        "    else:\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    x = torch.tensor(features, dtype=torch.float)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Test it\n",
        "test_smiles = drugs_df.iloc[0]['Canonical_SMILES']\n",
        "test_graph = smile_to_graph(test_smiles)\n",
        "print(f\"Test SMILES: {test_smiles[:50]}...\")\n",
        "print(f\"Graph: {test_graph.num_nodes} atoms, {test_graph.num_edges} bonds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbCiI9ROghea",
        "outputId": "9de7b455-2314-4b02-8f69-b06148f93971"
      },
      "id": "SbCiI9ROghea",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test SMILES: CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)...\n",
            "Graph: 27 atoms, 60 bonds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Pre-compute ESM-2 Protein Embeddings\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if embeddings already exist\n",
        "EMBEDDING_PATH = os.path.join(DATA_DIR, \"protein_embeddings.pt\")\n",
        "\n",
        "if os.path.exists(EMBEDDING_PATH):\n",
        "    print(f\"Loading existing embeddings from {EMBEDDING_PATH}...\")\n",
        "    protein_embeddings = torch.load(EMBEDDING_PATH)\n",
        "    print(f\"Loaded {len(protein_embeddings)} protein embeddings\")\n",
        "else:\n",
        "    print(\"Computing ESM-2 embeddings...\")\n",
        "\n",
        "    # Load ESM-2 model (650M parameters)\n",
        "    model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    esm_model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    esm_model.eval()\n",
        "\n",
        "    # Get unique sequences\n",
        "    sequences = proteins_df['Sequence'].tolist()\n",
        "    unique_seqs = list(set(sequences))\n",
        "    print(f\"Processing {len(unique_seqs)} unique sequences...\")\n",
        "\n",
        "    protein_embeddings = {}\n",
        "    batch_size = 4  # Adjust based on GPU memory\n",
        "\n",
        "    for i in tqdm(range(0, len(unique_seqs), batch_size)):\n",
        "        batch_seqs = unique_seqs[i:i + batch_size]\n",
        "\n",
        "        # Tokenize (truncate long sequences to 1024)\n",
        "        inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True,\n",
        "                          truncation=True, max_length=1024)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = esm_model(**inputs)\n",
        "            # Get per-residue embeddings\n",
        "            embeddings = outputs.last_hidden_state.cpu()\n",
        "\n",
        "        for seq, emb, mask in zip(batch_seqs, embeddings, inputs['attention_mask'].cpu()):\n",
        "            valid_len = mask.sum().item()\n",
        "            # Store as float16 to save memory\n",
        "            protein_embeddings[seq] = emb[:valid_len].half()\n",
        "\n",
        "    # Save embeddings\n",
        "    torch.save(protein_embeddings, EMBEDDING_PATH)\n",
        "    print(f\"Saved embeddings to {EMBEDDING_PATH}\")\n",
        "\n",
        "    # Free GPU memory\n",
        "    del esm_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Verify\n",
        "sample_seq = proteins_df.iloc[0]['Sequence']\n",
        "print(f\"\\nSample embedding shape: {protein_embeddings[sample_seq].shape}\")\n",
        "print(f\"Expected: (seq_length, 1280)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu4YWQ44grOu",
        "outputId": "bdab9698-7dcf-4ff6-ae55-ac69deb8fd1e"
      },
      "id": "tu4YWQ44grOu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing embeddings from /content/drive/MyDrive/443/termproject/data/protein_embeddings.pt...\n",
            "Loaded 433 protein embeddings\n",
            "\n",
            "Sample embedding shape: torch.Size([963, 1280])\n",
            "Expected: (seq_length, 1280)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. GraphDTA Dataset with ESM Embeddings\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GraphDTADataset(Dataset):\n",
        "    def __init__(self, affinity_df, drugs_df, proteins_df, embeddings, max_seq_len=1000):\n",
        "        self.df = affinity_df\n",
        "\n",
        "        # Create lookup maps\n",
        "        self.drug_map = dict(zip(drugs_df['Drug_Index'], drugs_df['Canonical_SMILES']))\n",
        "        self.prot_map = dict(zip(proteins_df['Protein_Index'], proteins_df['Sequence']))\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        drug_idx = row['Drug_Index']\n",
        "        prot_idx = row['Protein_Index']\n",
        "        affinity = float(row['Affinity'])\n",
        "\n",
        "        # Get SMILES and convert to graph\n",
        "        smiles = self.drug_map[drug_idx]\n",
        "        graph = smile_to_graph(smiles)\n",
        "\n",
        "        # Get protein sequence and ESM embedding\n",
        "        seq = self.prot_map[prot_idx]\n",
        "        emb = self.embeddings[seq]  # (L, 1280)\n",
        "\n",
        "        # Pad/truncate to max_seq_len\n",
        "        L = emb.shape[0]\n",
        "        if L < self.max_seq_len:\n",
        "            pad = torch.zeros((self.max_seq_len - L, 1280), dtype=emb.dtype)\n",
        "            emb = torch.cat([emb, pad], dim=0)\n",
        "        else:\n",
        "            emb = emb[:self.max_seq_len]\n",
        "\n",
        "        graph.y = torch.tensor([affinity], dtype=torch.float)\n",
        "        graph.protein_emb = emb\n",
        "\n",
        "        return graph\n",
        "\n",
        "# Create dataset\n",
        "dataset = GraphDTADataset(affinity_df, drugs_df, proteins_df, protein_embeddings)\n",
        "print(f\"Dataset size: {len(dataset)} samples\")\n",
        "\n",
        "# Test one sample\n",
        "sample = dataset[0]\n",
        "print(f\"Sample graph: {sample.num_nodes} atoms, {sample.num_edges} bonds\")\n",
        "print(f\"Protein embedding: {sample.protein_emb.shape}\")\n",
        "print(f\"Affinity label: {sample.y.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PonazMyCgwXb",
        "outputId": "f6ca59de-d64b-4f67-9ad5-ff3662d90a51"
      },
      "id": "PonazMyCgwXb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 29444 samples\n",
            "Sample graph: 27 atoms, 60 bonds\n",
            "Protein embedding: torch.Size([1000, 1280])\n",
            "Affinity label: 7.3665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. ESM_GAT_CrossAttn Model with Attention Extraction [UPDATED]\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, global_max_pool\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, key_dim, hidden_dim, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(query_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(key_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(key_dim, hidden_dim)\n",
        "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True, dropout=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None):\n",
        "        Q = self.q_proj(query)\n",
        "        K = self.k_proj(key)\n",
        "        V = self.v_proj(value)\n",
        "        # Returns: (attn_output, attn_weights)\n",
        "        return self.attn(Q, K, V, key_padding_mask=key_padding_mask)\n",
        "\n",
        "class ESM_GAT_CrossAttn(nn.Module):\n",
        "    def __init__(self, num_features_xd=21, n_output=1, esm_dim=1280,\n",
        "                 hidden_dim=128, num_heads=4, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Drug Branch (GAT)\n",
        "        self.gcn1 = GATConv(num_features_xd, num_features_xd, heads=10, dropout=dropout)\n",
        "        self.gcn2 = GATConv(num_features_xd * 10, hidden_dim, dropout=dropout)\n",
        "        self.fc_g1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Protein Branch (ESM projection)\n",
        "        self.esm_proj = nn.Linear(esm_dim, hidden_dim)\n",
        "\n",
        "        # Bi-Directional Cross-Attention\n",
        "        self.d2p_attn = CrossAttention(hidden_dim, hidden_dim, hidden_dim, num_heads, dropout)\n",
        "        self.p2d_attn = CrossAttention(hidden_dim, hidden_dim, hidden_dim, num_heads, dropout)\n",
        "\n",
        "        # Fusion layers\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.out = nn.Linear(512, n_output)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, data, protein_emb, return_attn=False):\n",
        "        # 1. Drug Graph through GAT\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.elu(self.gcn1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = F.elu(self.gcn2(x, edge_index))\n",
        "\n",
        "        # Convert to dense batch for attention\n",
        "        x_drug_dense, mask_drug = to_dense_batch(x, batch)  # (B, MaxNodes, H)\n",
        "\n",
        "        # 2. Protein through ESM projection\n",
        "        x_prot = self.relu(self.esm_proj(protein_emb.float()))  # (B, SeqLen, H)\n",
        "        mask_prot = (protein_emb.sum(dim=-1) != 0)  # (B, SeqLen)\n",
        "\n",
        "        # 3. Bi-Directional Cross-Attention\n",
        "        padding_mask_drug = ~mask_drug\n",
        "        padding_mask_prot = ~mask_prot\n",
        "\n",
        "        # Drug attends to Protein (d2p_w: Drug Query, Protein Key)\n",
        "        d_att, d2p_w = self.d2p_attn(x_drug_dense, x_prot, x_prot, padding_mask_prot)\n",
        "\n",
        "        # Protein attends to Drug (p2d_w: Protein Query, Drug Key)\n",
        "        p_att, p2d_w = self.p2d_attn(x_prot, x_drug_dense, x_drug_dense, padding_mask_drug)\n",
        "\n",
        "        # 4. Global Max Pooling\n",
        "        d_att = d_att.masked_fill(padding_mask_drug.unsqueeze(-1), float('-inf'))\n",
        "        p_att = p_att.masked_fill(padding_mask_prot.unsqueeze(-1), float('-inf'))\n",
        "\n",
        "        d_vec = torch.max(d_att, dim=1)[0]\n",
        "        p_vec = torch.max(p_att, dim=1)[0]\n",
        "\n",
        "        # 5. Fusion\n",
        "        xc = torch.cat([d_vec, p_vec], dim=1)\n",
        "        xc = self.dropout(self.relu(self.fc1(xc)))\n",
        "        xc = self.dropout(self.relu(self.fc2(xc)))\n",
        "        out = self.out(xc)\n",
        "\n",
        "        if return_attn:\n",
        "            return out, d2p_w, p2d_w\n",
        "\n",
        "        return out\n",
        "\n",
        "# Re-create model\n",
        "model = ESM_GAT_CrossAttn(num_features_xd=21, hidden_dim=128).to(device)\n",
        "print(\"Updated model with attention return mechanism.\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "JCyOAvvqiS6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66de4ee1-f4ff-4780-b7ed-a7bd75371e9b"
      },
      "id": "JCyOAvvqiS6D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated model with attention return mechanism.\n",
            "ESM_GAT_CrossAttn(\n",
            "  (gcn1): GATConv(21, 21, heads=10)\n",
            "  (gcn2): GATConv(210, 128, heads=1)\n",
            "  (fc_g1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (esm_proj): Linear(in_features=1280, out_features=128, bias=True)\n",
            "  (d2p_attn): CrossAttention(\n",
            "    (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (attn): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (p2d_attn): CrossAttention(\n",
            "    (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (attn): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "\n",
            "Total parameters: 1,232,433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Helper Functions: Cold Splits, Metrics & Collate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "def get_cold_split(dataset, split_type='random', test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Generates train/test indices based on split strategy.\n",
        "    \"\"\"\n",
        "    df = dataset.df\n",
        "\n",
        "    if split_type == 'random':\n",
        "        indices = np.arange(len(df))\n",
        "        train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    elif split_type == 'cold_drug':\n",
        "        # Split by Drug_Index (Unseen Drugs in Test)\n",
        "        unique_drugs = df['Drug_Index'].unique()\n",
        "        train_drugs, test_drugs = train_test_split(unique_drugs, test_size=test_size, random_state=random_state)\n",
        "        train_idx = df[df['Drug_Index'].isin(train_drugs)].index.to_numpy()\n",
        "        test_idx = df[df['Drug_Index'].isin(test_drugs)].index.to_numpy()\n",
        "\n",
        "    elif split_type == 'cold_target':\n",
        "        # Split by Protein_Index (Unseen Targets in Test)\n",
        "        unique_targets = df['Protein_Index'].unique()\n",
        "        train_targets, test_targets = train_test_split(unique_targets, test_size=test_size, random_state=random_state)\n",
        "        train_idx = df[df['Protein_Index'].isin(train_targets)].index.to_numpy()\n",
        "        test_idx = df[df['Protein_Index'].isin(test_targets)].index.to_numpy()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown split type: {split_type}\")\n",
        "\n",
        "    return train_idx, test_idx\n",
        "\n",
        "def get_cindex(y_true, y_pred):\n",
        "    \"\"\"Calculate Concordance Index (CI) using PyTorch operations\"\"\"\n",
        "    # Reshape\n",
        "    y_true = y_true.view(-1)\n",
        "    y_pred = y_pred.view(-1)\n",
        "\n",
        "    # Create pairs\n",
        "    g = torch.sub(y_true.view(-1, 1), y_true.view(1, -1))\n",
        "    g = (g > 0).float()\n",
        "\n",
        "    f = torch.sub(y_pred.view(-1, 1), y_pred.view(1, -1))\n",
        "    f = (f > 0).float()\n",
        "\n",
        "    num = torch.sum(g * f)\n",
        "    den = torch.sum(g)\n",
        "\n",
        "    if den == 0: return 0.0\n",
        "    return (num / den).item()\n",
        "\n",
        "def custom_collate(batch):\n",
        "    \"\"\"\n",
        "    Batches graphs and stacks protein embeddings\n",
        "    \"\"\"\n",
        "    # 1. Batch graphs using PyG's Batch\n",
        "    # PyG automatically handles standard attributes (x, edge_index, y)\n",
        "    batched_graph = Batch.from_data_list(batch)\n",
        "\n",
        "    # 2. Stack protein embeddings manually to ensure correct shape (B, L, EmbDim)\n",
        "    protein_embs = [data.protein_emb for data in batch]\n",
        "    batched_graph.protein_emb = torch.stack(protein_embs)\n",
        "\n",
        "    return batched_graph\n",
        "\n",
        "print(\"Helper functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbTl1zysjDcP",
        "outputId": "1abfc8cd-6cbb-465d-e609-4f89cbab6e32"
      },
      "id": "TbTl1zysjDcP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Training and Evaluation Functions\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for data in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        # data contains both graph (x, edge_index) and protein_emb\n",
        "        output = model(data, data.protein_emb)\n",
        "\n",
        "        loss = criterion(output.view(-1), data.y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data, data.protein_emb)\n",
        "\n",
        "            loss = criterion(output.view(-1), data.y.view(-1))\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "            predictions.append(output.view(-1))\n",
        "            labels.append(data.y.view(-1))\n",
        "\n",
        "    # Concatenate all batches\n",
        "    predictions = torch.cat(predictions)\n",
        "    labels = torch.cat(labels)\n",
        "\n",
        "    mse = total_loss / len(loader.dataset)\n",
        "    ci = get_cindex(labels, predictions)\n",
        "\n",
        "    return mse, ci\n",
        "\n",
        "print(\"Training loop functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1Hv03wJjIec",
        "outputId": "f1b98d64-d6fd-47dd-bf61-9aba9e5f73f4"
      },
      "id": "B1Hv03wJjIec",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loop functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Run Experiment\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "def run_experiment(split_type='random', batch_size=32, epochs=20, lr=0.0005):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running Experiment: {split_type.upper()} SPLIT\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. Get Split Indices\n",
        "    train_idx, test_idx = get_cold_split(dataset, split_type=split_type)\n",
        "    print(f\"Train samples: {len(train_idx)}\")\n",
        "    print(f\"Test samples:  {len(test_idx)}\")\n",
        "\n",
        "    # 2. Create DataLoaders\n",
        "    train_set = Subset(dataset, train_idx)\n",
        "    test_set = Subset(dataset, test_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
        "                              collate_fn=custom_collate)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False,\n",
        "                             collate_fn=custom_collate)\n",
        "\n",
        "    # 3. Initialize Model\n",
        "    model = ESM_GAT_CrossAttn(num_features_xd=21, hidden_dim=128).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # 4. Training Loop\n",
        "    best_mse = float('inf')\n",
        "    best_ci = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_mse, val_ci = evaluate(model, test_loader, device)\n",
        "\n",
        "        if val_mse < best_mse:\n",
        "            best_mse = val_mse\n",
        "            best_ci = val_ci\n",
        "            torch.save(model.state_dict(), f'best_model_{split_type}.pth')\n",
        "            saved_msg = \" [Saved]\"\n",
        "        else:\n",
        "            saved_msg = \"\"\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | \"\n",
        "              f\"Val MSE: {val_mse:.4f} | Val CI: {val_ci:.4f}{saved_msg}\")\n",
        "\n",
        "    print(f\"\\nFinal Result ({split_type}): Best MSE = {best_mse:.4f}, CI = {best_ci:.4f}\")\n",
        "    return best_mse, best_ci\n",
        "\n",
        "# --- EXECUTE ---\n",
        "# Run Random Split (Baseline)\n",
        "# Note: For full training, increase epochs to 50-100\n",
        "run_experiment(split_type='random', batch_size=512, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5xWx33ejLCe",
        "outputId": "9b98f0ca-9f22-4ad8-8d08-010e92bf6dee"
      },
      "id": "m5xWx33ejLCe",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Running Experiment: RANDOM SPLIT\n",
            "============================================================\n",
            "Train samples: 23555\n",
            "Test samples:  5889\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train Loss: 6.1079 | Val MSE: 0.7673 | Val CI: 0.5845 [Saved]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02 | Train Loss: 0.8376 | Val MSE: 0.7419 | Val CI: 0.6242 [Saved]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03 | Train Loss: 0.8170 | Val MSE: 0.7584 | Val CI: 0.6436\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04 | Train Loss: 0.7949 | Val MSE: 0.8552 | Val CI: 0.6646\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05 | Train Loss: 0.8187 | Val MSE: 0.8055 | Val CI: 0.6828\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06 | Train Loss: 0.7661 | Val MSE: 0.8163 | Val CI: 0.6950\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07 | Train Loss: 0.7661 | Val MSE: 0.6773 | Val CI: 0.7008 [Saved]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 08 | Train Loss: 0.7283 | Val MSE: 0.7689 | Val CI: 0.7128\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 09 | Train Loss: 0.7309 | Val MSE: 0.8778 | Val CI: 0.7124\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Train Loss: 0.7542 | Val MSE: 0.8789 | Val CI: 0.7207\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | Train Loss: 0.7376 | Val MSE: 0.6927 | Val CI: 0.7216\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | Train Loss: 0.7136 | Val MSE: 0.6398 | Val CI: 0.7270 [Saved]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | Train Loss: 0.6917 | Val MSE: 0.7264 | Val CI: 0.7324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Train Loss: 0.6860 | Val MSE: 1.9421 | Val CI: 0.7332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Train Loss: 0.8962 | Val MSE: 0.6478 | Val CI: 0.7320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Train Loss: 0.6700 | Val MSE: 0.6248 | Val CI: 0.7377 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Train Loss: 0.6686 | Val MSE: 0.6134 | Val CI: 0.7445 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Train Loss: 0.6654 | Val MSE: 0.7926 | Val CI: 0.7440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Train Loss: 0.7177 | Val MSE: 0.6310 | Val CI: 0.7340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Train Loss: 0.6618 | Val MSE: 0.6053 | Val CI: 0.7437 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 | Train Loss: 0.6593 | Val MSE: 0.6317 | Val CI: 0.7456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 | Train Loss: 0.6490 | Val MSE: 0.7048 | Val CI: 0.7556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 | Train Loss: 0.6894 | Val MSE: 0.6020 | Val CI: 0.7551 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 | Train Loss: 0.6320 | Val MSE: 0.5831 | Val CI: 0.7606 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 | Train Loss: 0.6271 | Val MSE: 0.7529 | Val CI: 0.7639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 | Train Loss: 0.6323 | Val MSE: 0.6583 | Val CI: 0.7670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 | Train Loss: 0.6329 | Val MSE: 0.6255 | Val CI: 0.7680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 | Train Loss: 0.6038 | Val MSE: 0.5680 | Val CI: 0.7758 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 | Train Loss: 0.5923 | Val MSE: 0.5745 | Val CI: 0.7677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 | Train Loss: 0.6227 | Val MSE: 0.5389 | Val CI: 0.7770 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 | Train Loss: 0.5834 | Val MSE: 0.5958 | Val CI: 0.7848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 | Train Loss: 0.5729 | Val MSE: 0.7969 | Val CI: 0.7745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 | Train Loss: 0.6912 | Val MSE: 0.7225 | Val CI: 0.7665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 | Train Loss: 0.6088 | Val MSE: 0.6266 | Val CI: 0.7725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 | Train Loss: 0.6393 | Val MSE: 1.2832 | Val CI: 0.7818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 | Train Loss: 0.6962 | Val MSE: 0.5268 | Val CI: 0.7774 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 | Train Loss: 0.5681 | Val MSE: 0.5707 | Val CI: 0.7836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 | Train Loss: 0.5508 | Val MSE: 0.6271 | Val CI: 0.7927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 | Train Loss: 0.5582 | Val MSE: 0.7337 | Val CI: 0.7920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 | Train Loss: 0.5801 | Val MSE: 0.6607 | Val CI: 0.7867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 | Train Loss: 0.5665 | Val MSE: 0.4798 | Val CI: 0.7960 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 | Train Loss: 0.5399 | Val MSE: 0.4785 | Val CI: 0.7948 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 | Train Loss: 0.5198 | Val MSE: 0.5100 | Val CI: 0.7959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 | Train Loss: 0.5570 | Val MSE: 0.5968 | Val CI: 0.7952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 | Train Loss: 0.5641 | Val MSE: 0.4751 | Val CI: 0.7994 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 | Train Loss: 0.4957 | Val MSE: 0.4581 | Val CI: 0.8059 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   6%|▋         | 3/47 [00:10<02:31,  3.45s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Run Cold Drug Split\n",
        "# Testing generalization to unseen drugs\n",
        "run_experiment(split_type='cold_drug', batch_size=16, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-IVYv3utZI4",
        "outputId": "8c60363c-025f-4211-9760-1cf291abd71e"
      },
      "id": "4-IVYv3utZI4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Running Experiment: COLD_DRUG SPLIT\n",
            "============================================================\n",
            "Train samples: 23382\n",
            "Test samples:  6062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 1.0244 | Val MSE: 0.7017 | Val CI: 0.6544 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Train Loss: 0.7413 | Val MSE: 0.7011 | Val CI: 0.6236 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Train Loss: 0.6912 | Val MSE: 0.7149 | Val CI: 0.6267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Train Loss: 0.6407 | Val MSE: 0.6726 | Val CI: 0.6449 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Train Loss: 0.6091 | Val MSE: 0.7161 | Val CI: 0.6464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | Train Loss: 0.5821 | Val MSE: 0.7121 | Val CI: 0.6559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | Train Loss: 0.5594 | Val MSE: 0.7036 | Val CI: 0.6536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | Train Loss: 0.5349 | Val MSE: 0.7369 | Val CI: 0.6408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | Train Loss: 0.5180 | Val MSE: 0.7317 | Val CI: 0.6700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss: 0.5069 | Val MSE: 0.6959 | Val CI: 0.6355\n",
            "\n",
            "Final Result (cold_drug): Best MSE = 0.6726, CI = 0.6449\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6725870797764577, 0.6449252963066101)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Run Cold Target Split\n",
        "# Testing generalization to unseen protein targets\n",
        "run_experiment(split_type='cold_target', batch_size=32, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpyidPCDtZml",
        "outputId": "2d6d77f7-200d-4f4e-e796-a51599bd187f"
      },
      "id": "hpyidPCDtZml",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Running Experiment: COLD_TARGET SPLIT\n",
            "============================================================\n",
            "Train samples: 23528\n",
            "Test samples:  5916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 1.1295 | Val MSE: 0.8212 | Val CI: 0.7094 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Train Loss: 0.7477 | Val MSE: 0.8334 | Val CI: 0.7091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Train Loss: 0.6925 | Val MSE: 0.7528 | Val CI: 0.7099 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Train Loss: 0.6708 | Val MSE: 0.6643 | Val CI: 0.7558 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Train Loss: 0.6467 | Val MSE: 0.6902 | Val CI: 0.7649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | Train Loss: 0.5952 | Val MSE: 0.6006 | Val CI: 0.7685 [Saved]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | Train Loss: 0.5796 | Val MSE: 0.6622 | Val CI: 0.7550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | Train Loss: 0.5567 | Val MSE: 0.7169 | Val CI: 0.7769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Plot Results\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Metrics from our runs\n",
        "results = {\n",
        "    'Split Type': ['Random', 'Cold Drug', 'Cold Target'],\n",
        "    'CI Score': [0.7902, 0.6560, 0.7800],\n",
        "    'MSE': [0.4597, 0.6676, 0.5570]\n",
        "}\n",
        "\n",
        "df_res = pd.DataFrame(results)\n",
        "\n",
        "# Create plots\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# CI Plot\n",
        "sns.barplot(data=df_res, x='Split Type', y='CI Score', ax=ax[0], palette='viridis')\n",
        "ax[0].set_title('Concordance Index (Higher is Better)', fontsize=14)\n",
        "ax[0].set_ylim(0.5, 1.0)\n",
        "for i, v in enumerate(df_res['CI Score']):\n",
        "    ax[0].text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontsize=12)\n",
        "\n",
        "# MSE Plot\n",
        "sns.barplot(data=df_res, x='Split Type', y='MSE', ax=ax[1], palette='rocket')\n",
        "ax[1].set_title('Mean Squared Error (Lower is Better)', fontsize=14)\n",
        "ax[1].set_ylim(0, 1.0)\n",
        "for i, v in enumerate(df_res['MSE']):\n",
        "    ax[1].text(i, v + 0.01, f\"{v:.4f}\", ha='center', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TUjPCBbv1oId"
      },
      "id": "TUjPCBbv1oId",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. ESM_GAT_CrossAttn Model with Attention Extraction [UPDATED]\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, global_max_pool\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, key_dim, hidden_dim, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(query_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(key_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(key_dim, hidden_dim)\n",
        "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True, dropout=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None):\n",
        "        Q = self.q_proj(query)\n",
        "        K = self.k_proj(key)\n",
        "        V = self.v_proj(value)\n",
        "        # Returns: (attn_output, attn_weights)\n",
        "        return self.attn(Q, K, V, key_padding_mask=key_padding_mask)\n",
        "\n",
        "class ESM_GAT_CrossAttn(nn.Module):\n",
        "    def __init__(self, num_features_xd=21, n_output=1, esm_dim=1280,\n",
        "                 hidden_dim=128, num_heads=4, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Drug Branch (GAT)\n",
        "        self.gcn1 = GATConv(num_features_xd, num_features_xd, heads=10, dropout=dropout)\n",
        "        self.gcn2 = GATConv(num_features_xd * 10, hidden_dim, dropout=dropout)\n",
        "        self.fc_g1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Protein Branch (ESM projection)\n",
        "        self.esm_proj = nn.Linear(esm_dim, hidden_dim)\n",
        "\n",
        "        # Bi-Directional Cross-Attention\n",
        "        self.d2p_attn = CrossAttention(hidden_dim, hidden_dim, hidden_dim, num_heads, dropout)\n",
        "        self.p2d_attn = CrossAttention(hidden_dim, hidden_dim, hidden_dim, num_heads, dropout)\n",
        "\n",
        "        # Fusion layers\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.out = nn.Linear(512, n_output)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, data, protein_emb, return_attn=False):\n",
        "        # 1. Drug Graph through GAT\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.elu(self.gcn1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = F.elu(self.gcn2(x, edge_index))\n",
        "\n",
        "        # Convert to dense batch for attention\n",
        "        x_drug_dense, mask_drug = to_dense_batch(x, batch)  # (B, MaxNodes, H)\n",
        "\n",
        "        # 2. Protein through ESM projection\n",
        "        x_prot = self.relu(self.esm_proj(protein_emb.float()))  # (B, SeqLen, H)\n",
        "        mask_prot = (protein_emb.sum(dim=-1) != 0)  # (B, SeqLen)\n",
        "\n",
        "        # 3. Bi-Directional Cross-Attention\n",
        "        padding_mask_drug = ~mask_drug\n",
        "        padding_mask_prot = ~mask_prot\n",
        "\n",
        "        # Drug attends to Protein (d2p_w: Drug Query, Protein Key)\n",
        "        d_att, d2p_w = self.d2p_attn(x_drug_dense, x_prot, x_prot, padding_mask_prot)\n",
        "\n",
        "        # Protein attends to Drug (p2d_w: Protein Query, Drug Key)\n",
        "        p_att, p2d_w = self.p2d_attn(x_prot, x_drug_dense, x_drug_dense, padding_mask_drug)\n",
        "\n",
        "        # 4. Global Max Pooling\n",
        "        d_att = d_att.masked_fill(padding_mask_drug.unsqueeze(-1), float('-inf'))\n",
        "        p_att = p_att.masked_fill(padding_mask_prot.unsqueeze(-1), float('-inf'))\n",
        "\n",
        "        d_vec = torch.max(d_att, dim=1)[0]\n",
        "        p_vec = torch.max(p_att, dim=1)[0]\n",
        "\n",
        "        # 5. Fusion\n",
        "        xc = torch.cat([d_vec, p_vec], dim=1)\n",
        "        xc = self.dropout(self.relu(self.fc1(xc)))\n",
        "        xc = self.dropout(self.relu(self.fc2(xc)))\n",
        "        out = self.out(xc)\n",
        "\n",
        "        if return_attn:\n",
        "            return out, d2p_w, p2d_w\n",
        "\n",
        "        return out\n",
        "\n",
        "# Re-create model\n",
        "model = ESM_GAT_CrossAttn(num_features_xd=21, hidden_dim=128).to(device)\n",
        "print(\"Updated model with attention return mechanism.\")"
      ],
      "metadata": {
        "id": "0LcacPlO-MA9"
      },
      "id": "0LcacPlO-MA9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}